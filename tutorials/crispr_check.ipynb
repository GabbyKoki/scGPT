{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Extracting zip file...\n",
      "Done!\n",
      "Creating pyg object for each cell in the data...\n",
      "100%|██████████| 87/87 [01:04<00:00,  1.36it/s]\n",
      "Saving new dataset pyg object at ./data/adamson/data_pyg/cell_graphs.pkl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gears import PertData\n",
    "\n",
    "# Load the Adamson dataset\n",
    "pert_data = PertData(\"./data\")\n",
    "pert_data.load(data_name=\"adamson\")\n",
    "pert_data.prepare_split(split=\"simulation\", seed=1)\n",
    "pert_data.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.tokenizer import GeneVocab\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab = GeneVocab.from_file(\"path/to/vocab.json\")\n",
    "\n",
    "# Load model configuration\n",
    "with open(\"path/to/model_config.json\", \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "\n",
    "# Create the model\n",
    "model = TransformerGenerator(\n",
    "    ntokens=len(vocab),\n",
    "    embsize=model_configs[\"embsize\"],\n",
    "    nhead=model_configs[\"nheads\"],\n",
    "    d_hid=model_configs[\"d_hid\"],\n",
    "    nlayers=model_configs[\"nlayers\"],\n",
    "    nlayers_cls=model_configs[\"n_layers_cls\"],\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=model_configs[\"dropout\"],\n",
    "    pad_token=model_configs[\"pad_token\"],\n",
    "    pad_value=model_configs[\"pad_value\"],\n",
    "    do_mvc=True,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_state_dict(torch.load(\"path/to/pretrained_model.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CellBehaviorClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Assuming we have 3 cell behavior classes\n",
    "classifier = CellBehaviorClassifier(input_dim=model_configs[\"embsize\"], num_classes=3)\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Combine scGPT and classifier\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, scgpt, classifier):\n",
    "        super().__init__()\n",
    "        self.scgpt = scgpt\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, input_gene_ids, input_values, input_pert_flags):\n",
    "        scgpt_output = self.scgpt(input_gene_ids, input_values, input_pert_flags)\n",
    "        cell_embedding = scgpt_output[\"cell_embedding\"]\n",
    "        classifier_output = self.classifier(cell_embedding)\n",
    "        return classifier_output\n",
    "\n",
    "combined_model = CombinedModel(model, classifier)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    for batch in pert_data.dataloader[\"train_loader\"]:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_gene_ids = batch.genes.to(device)\n",
    "        input_values = batch.expressions.to(device)\n",
    "        input_pert_flags = batch.pert_flags.to(device)\n",
    "        \n",
    "        # Assuming we have cell behavior labels\n",
    "        labels = batch.cell_behavior_labels.to(device)\n",
    "        \n",
    "        outputs = combined_model(input_gene_ids, input_values, input_pert_flags)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "combined_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in pert_data.dataloader[\"test_loader\"]:\n",
    "        input_gene_ids = batch.genes.to(device)\n",
    "        input_values = batch.expressions.to(device)\n",
    "        input_pert_flags = batch.pert_flags.to(device)\n",
    "        labels = batch.cell_behavior_labels.to(device)\n",
    "        \n",
    "        outputs = combined_model(input_gene_ids, input_values, input_pert_flags)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(combined_model.state_dict(), \"path/to/fine_tuned_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 0\n",
    "include_zero_gene = \"all\"\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "amp = True\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "epochs = 15\n",
    "schedule_interval = 1\n",
    "early_stop = 10\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512\n",
    "d_hid = 512\n",
    "nlayers = 12\n",
    "nhead = 8\n",
    "n_layers_cls = 3\n",
    "dropout = 0\n",
    "use_fast_transformer = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create vocabulary\u001b[39;00m\n\u001b[1;32m      2\u001b[0m genes \u001b[38;5;241m=\u001b[39m pert_data\u001b[38;5;241m.\u001b[39madata\u001b[38;5;241m.\u001b[39mvar[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 3\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mVocab\u001b[49m(\n\u001b[1;32m      4\u001b[0m     VocabPybind(genes \u001b[38;5;241m+\u001b[39m special_tokens, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m gene_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m      8\u001b[0m     [vocab[gene] \u001b[38;5;28;01mif\u001b[39;00m gene \u001b[38;5;129;01min\u001b[39;00m vocab \u001b[38;5;28;01melse\u001b[39;00m vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m gene \u001b[38;5;129;01min\u001b[39;00m genes], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "vocab = Vocab(\n",
    "    VocabPybind(genes + special_tokens, None)\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)\n",
    "\n",
    "# Initialize the model\n",
    "ntokens = len(vocab)\n",
    "model = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = masked_mse_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, schedule_interval, gamma=0.9)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
